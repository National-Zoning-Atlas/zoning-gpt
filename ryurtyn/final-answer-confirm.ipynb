{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    async def extract(\\n        self, pages: list[LookupOutput], district: District, term: str, town: str\\n    ):\\n        # We first map extraction across all pages.\\n        results = []\\n        empty_results = []\\n        map_pickle = []\\n        async for r in super().extract(pages, district, term, town):\\n            map_pickle.append(r)\\n            if (r.output is not None) and r.output.extracted_text:\\n                results.append(r)\\n            else:\\n                empty_results.append(r)\\n                \\n        # Load existing data if the file already exists\\n        try:\\n            with open(\"./ryurtyn/map_results_11_14_1.dat\", \"rb\") as f:\\n                existing_data = pickle.load(f)\\n        except FileNotFoundError:\\n            existing_data = {}  # If the file doesn\\'t exist, initialize an empty dictionary\\n\\n        try:\\n            with open(\"./ryurtyn/districts_11_14_1.dat\", \"rb\") as f:\\n                existing_data_district = pickle.load(f)\\n        except FileNotFoundError:\\n            existing_data_district = {} \\n        # Assuming district.short_name and results are defined somewhere in your code\\n        new_data = {town+\"-\"+district.short_name: map_pickle}\\n        new_data_district = {town+\"-\"+district.short_name: district}\\n\\n        # Merge the existing data with the new data\\n        existing_data.update(new_data)\\n        existing_data_district.update(new_data_district)\\n\\n        # Write the updated data back to the file\\n        with open(\"./ryurtyn/map_results_11_14_1.dat\", \"wb\") as f:\\n            pickle.dump(existing_data, f)\\n\\n        with open(\"./ryurtyn/districts_11_14_1.dat\", \"wb\") as f:\\n            pickle.dump(existing_data_district, f)\\n\\n        # Ensure that we yield one empty result to handle case when the expected output is None\\n        if len(empty_results) != 0:\\n            yield empty_results[0]\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If you want to use new map results, then go to tournament_reduce.py, and replace the extract() method with the following code. \n",
    "This will save the map results for use in this notebook. Once those are saved, switch the original code back. \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "    async def extract(\n",
    "        self, pages: list[LookupOutput], district: District, term: str, town: str\n",
    "    ):\n",
    "        # We first map extraction across all pages.\n",
    "        results = []\n",
    "        empty_results = []\n",
    "        map_pickle = []\n",
    "        async for r in super().extract(pages, district, term, town):\n",
    "            map_pickle.append(r)\n",
    "            if (r.output is not None) and r.output.extracted_text:\n",
    "                results.append(r)\n",
    "            else:\n",
    "                empty_results.append(r)\n",
    "                \n",
    "        # Load existing data if the file already exists\n",
    "        try:\n",
    "            with open(\"./ryurtyn/map_results_11_14_1.dat\", \"rb\") as f:\n",
    "                existing_data = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            existing_data = {}  # If the file doesn't exist, initialize an empty dictionary\n",
    "\n",
    "        try:\n",
    "            with open(\"./ryurtyn/districts_11_14_1.dat\", \"rb\") as f:\n",
    "                existing_data_district = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            existing_data_district = {} \n",
    "        # Assuming district.short_name and results are defined somewhere in your code\n",
    "        new_data = {town+\"-\"+district.short_name: map_pickle}\n",
    "        new_data_district = {town+\"-\"+district.short_name: district}\n",
    "\n",
    "        # Merge the existing data with the new data\n",
    "        existing_data.update(new_data)\n",
    "        existing_data_district.update(new_data_district)\n",
    "\n",
    "        # Write the updated data back to the file\n",
    "        with open(\"./ryurtyn/map_results_11_14_1.dat\", \"wb\") as f:\n",
    "            pickle.dump(existing_data, f)\n",
    "\n",
    "        with open(\"./ryurtyn/districts_11_14_1.dat\", \"wb\") as f:\n",
    "            pickle.dump(existing_data_district, f)\n",
    "\n",
    "        # Ensure that we yield one empty result to handle case when the expected output is None\n",
    "        if len(empty_results) != 0:\n",
    "            yield empty_results[0]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rusy/Documents/Cornell/fa23/Zoning/zoning-gpt/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from zoning.term_extraction.extract.answer_confirm_tester import AnswerConfirmTester, answer_confirm_test\n",
    "from zoning.term_extraction.eval_results import clean_string_units\n",
    "from zoning.term_extraction.extract.utils import include_context_around_phrase\n",
    "import asyncio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the map outputs\n",
    "with open(\"map_results_11_14_3.dat\", \"rb\") as f:\n",
    "    map_outputs = pickle.load(f)\n",
    "\n",
    "# load the dictionary of district objects\n",
    "with open(\"districts_11_14_3.dat\", \"rb\") as f:\n",
    "    districts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv(\"../data/ground_truth.csv\")\n",
    "gt_first_30 = gt.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_value_exists = gt_first_30[gt_first_30[\"min_lot_size_gt\"].notnull()]\n",
    "gt_value_not_exist = gt_first_30[gt_first_30[\"min_lot_size_gt\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function that executes the coroutine object\n",
    "async def execute_answer_confirm_test(inputs, term, district, town):\n",
    "    # Your coroutine object (replace this with your actual coroutine)\n",
    "    async def my_coroutine():\n",
    "        return await answer_confirm_test(inputs, term, district, 1, town)\n",
    "    # Execute the coroutine object using await\n",
    "    return await my_coroutine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        Andover Lake\n",
       "1                        AA Residence\n",
       "2              Technology Development\n",
       "3                     Industrial Park\n",
       "4                 Restricted Business\n",
       "5                    General Business\n",
       "6                  General Industrial\n",
       "7                     Village Overlay\n",
       "8     Housing Opportunity Development\n",
       "9                          Commercial\n",
       "10               I-1 General Industry\n",
       "11                    General Buiness\n",
       "12                      Industrial-30\n",
       "13                      Residence R-3\n",
       "14              Multifamily Residence\n",
       "15                                A-1\n",
       "16                         Business C\n",
       "17                         Commercial\n",
       "18                   Residential R-40\n",
       "19      Town Center Perimeter Overlay\n",
       "20                         Industrial\n",
       "21                Restricted Business\n",
       "22                         Industrial\n",
       "23                     Rural Business\n",
       "24                 Village Commercial\n",
       "25                    R-3 Residential\n",
       "26                 Aquifer Protection\n",
       "27                  Residential R-20A\n",
       "28                     Tidal Wetlands\n",
       "29                         Business-2\n",
       "Name: district, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_first_30[\"district\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bethel', 'bridgewater'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_first_30[gt_first_30[\"district\"] == \"Commercial\"][\"town\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andover Lake andover\n",
      "AA Residence ansonia\n",
      "Technology Development ashford\n",
      "Industrial Park avon\n",
      "Restricted Business barkhamsted\n"
     ]
    }
   ],
   "source": [
    "for d, t in zip(gt_first_30[\"district\"][:5], gt_first_30[\"town\"][:5]):\n",
    "    print(d, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "District:  full_name='Industrial' short_name='IND'\n",
      "GT answer:  87120 Cleaned Answer:  [87120.0] Raw Answer:  2 acres\n",
      "GT answer:  87120 Cleaned Answer:  [10000.0] Raw Answer:  10,000 sq ft\n",
      "GT answer:  87120 Cleaned Answer:  [20000.0] Raw Answer:  20,000 sq ft\n",
      "GT answer:  87120 Cleaned Answer:  [30000.0] Raw Answer:  30,000 sq ft\n",
      "GT answer:  87120 Cleaned Answer:  [87120.0] Raw Answer:  87,120 sq ft\n",
      "GT answer:  87120 Cleaned Answer:  [87120.0] Raw Answer:  2 acres\n",
      "ANSWER:  True Y\n",
      "ANSWER:  False N\n",
      "ANSWER:  False N\n",
      "ANSWER:  False N\n",
      "ANSWER:  True Y\n",
      "ANSWER:  True Y\n",
      "Number Correct:  6\n",
      "Number Incorrect:  0\n",
      "Accuracy:  1.0\n",
      "Final Num Correct:  6\n",
      "Final Num Incorrect:  0\n",
      "Final Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "total_correct = 0\n",
    "total_incorrect = 0\n",
    "\n",
    "weird_districts = [\"I-1 General Industry\", \"General Buiness\"]\n",
    "null_answer_types = set([\"N/A\", \"None\", \"Null\", \"n/a\", \"none\", \"null\", \"\"])\n",
    "# for d in gt_first_30[\"district\"][:5]:\n",
    "# for d in [\"AA Residence\", \"Technology Development\", \"Industrial Park\", \"Restricted Business\", \"General Business\", \"Planned Adaptive Reuse Development 3\", \"General Industrial\", \"Village Overlay\", \"Housing Opportunity Development\", \"Industrial-30\"]:\n",
    "# for d, t in zip(gt_first_30[\"district\"][:30], gt_first_30[\"town\"][:30]):\n",
    "for t, d in [(\"burlington\", \"Industrial\")]:\n",
    "# for t, d in [(\"beacon-falls\", \"Planned Adaptive Reuse Development 3\")]:\n",
    "    town = gt_first_30[gt_first_30[\"district\"] == d][\"town\"].values[0]\n",
    "    district_short_name = gt_first_30[gt_first_30[\"district\"] == d][\"district_abb\"].values[0]\n",
    "    district = districts[town+\"-\"+district_short_name]\n",
    "    inputs = map_outputs[town+\"-\"+district_short_name]\n",
    "    town = gt_first_30[gt_first_30[\"district\"] == d][\"town\"].values[0]\n",
    "    term = \"min_lot_size\"\n",
    "    print(\"District: \", district)\n",
    "\n",
    "    true_val = gt_first_30[gt_first_30[\"district\"] == d][\"min_lot_size_gt\"].values[0]\n",
    "    try:\n",
    "        true_val = int(true_val)\n",
    "    except ValueError:\n",
    "        print(\"skipping \", d, \"true value is\", true_val)\n",
    "        true_val = \"\"\n",
    "        # continue\n",
    "    true_indices = []\n",
    "    for a in range(len(inputs)):\n",
    "        i = inputs[a]\n",
    "        if i.output and i.output.answer:\n",
    "            # print(i.output.answer)\n",
    "            i_answer_clean = clean_string_units(i.output.answer)\n",
    "            print(\"GT answer: \", true_val, \"Cleaned Answer: \", i_answer_clean, \"Raw Answer: \", i.output.answer)\n",
    "            if i_answer_clean:\n",
    "                i_answer_clean = int(i_answer_clean[0])\n",
    "                if i_answer_clean == true_val:\n",
    "                    true_indices.append(a)\n",
    "                    continue\n",
    "            if (not true_val) and (i.output.answer in null_answer_types):\n",
    "                true_indices.append(a)\n",
    "                continue\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_incorrect = 0\n",
    "    curr_length = 0\n",
    "    \n",
    "    for a in range(len(inputs)):\n",
    "        i = inputs[a]\n",
    "        if i.output and i.output.extracted_text:\n",
    "            curr_length += 1\n",
    "            \n",
    "            res = await execute_answer_confirm_test(i, term, district, town)\n",
    "            if a in true_indices:\n",
    "                if res == \"Y\":\n",
    "                    num_correct += 1\n",
    "                else:\n",
    "                    num_incorrect += 1\n",
    "            else:\n",
    "                if res == \"N\":\n",
    "                    num_correct += 1\n",
    "                else:\n",
    "                    num_incorrect += 1\n",
    "            print(\"ANSWER: \", a in true_indices, res)\n",
    "    total_correct += num_correct\n",
    "    total_incorrect += num_incorrect\n",
    "    total_length += curr_length\n",
    "                    \n",
    "\n",
    "    print(\"Number Correct: \", num_correct)\n",
    "    print(\"Number Incorrect: \", num_incorrect)\n",
    "    print(\"Accuracy: \", num_correct / curr_length)\n",
    "\n",
    "print(\"Final Num Correct: \", total_correct)\n",
    "print(\"Final Num Incorrect: \", total_incorrect)\n",
    "print(\"Final Accuracy: \", total_correct / total_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tldr: if the answer is \"N\" we should return a null answer entry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
